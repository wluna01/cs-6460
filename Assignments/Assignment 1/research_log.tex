\documentclass[
	%a4paper, % Use A4 paper size
	letterpaper, % Use US letter paper size
]{jdf}
\addbibresource{../../references.bib}
\author{William Luna}
\email{wluna6@gatech.edu}
\title{Assignment 1: Research Log}

\begin{document}
%\lsstyle

\maketitle

\section{Background}
%In about half a page, summarize your current state. For this Research Log, this is likely a summary of your prior interests. If you already know what you are interested in doing, you’d write about that; if you don’t, you’d write about your more general interests in Educational Technology.

I'm starting the term with a clear idea of what want to make, but with a cursory understanding of the research that surrounds it.

As an avid language learner, I've noticed that most products–from consumer app to community college course–are focused on active learning. I've often felt a lack of attractive options for passive learning, such as watching TV or reading a book, makes it difficult to take study during parts of the day when most people have lower motivation. Of course, most languages have no shortage of media to engage with. However, most of this media is either geared at native speakers, making it effortful to comprehend as a student, or "graded content" that is helpful but difficult to acquire in sufficient volume for the student's current level.

An additional wrinkle is that students of a foreign language have higher variability in their background and proficiency than native speakers. If a touring musician has near-native English vocabulary to describe music but has never used the language to talk about computers, they need a very different IT lesson than someone who works in the tech industry.

These are two challenges I have observed in teaching and learning Spanish, Portuguese, and Mandarin Chinese. They lead me to wonder: can a Large Language Model be leveraged to generate easily comprehensible input, trained on each individual's vocabulary, to create content that makes passive learning of a foreign language more attractive?

\section{Papers}
%As you walk through the Research Guide, you’ll be finding lots of papers to read. Here, you’ll make a list of the papers you come across and give considerable attention to. We would expect the Research Log to include at least 15-20 sources (though more is fine as well), and at least 12 (preferably more) should be academic and peer-reviewed. You may include blog posts, newspaper articles, etc. as well, but you should have at least 12 academic sources, too.

\subsection{\fullcite{hu_2000}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
This paper was part of the curriculum for the course \textit{Linguistic Approaches to Second Language Acquisition} that I took in undergrad.

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
This paper provides evidence that a reader has to know ninety-eight percent of the words in a text to be able to both comprehend its meaning and acquire new vocabulary (the remaining two percent) passively through context, without referring to any external tool such as a dictionary. The literature review provides different mental models of vocabulary acquisition. Of note is the observation that native speakers of a language generally learn to read with texts that only contain words they already know, compared to adult learners of a foreign language who will generally begin reading before establishing a high vocabulary. It also points out that the frequency of uncommon words is generally higher in English non-fiction than fiction, making fiction a better medium for learners with limited vocabulary. The study then seeks to empirically determine the "sweet spot" of unknown words in a text, where enough unknown words are present to create opportunity for passive acquisition of vocabulary, without so many unknown words as to overwhelm or exhaust the reader. The paper concludes that ninety-eight percent of the words in a text should be known to achieve this sweet spot.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
The key takeaway is that any system that seeks to generate text to optimize passive vocabulary acquisition should strive for ninety-eight percent comprehension. It also suggests that it is easier to generating fiction that achieves this threshold than non-fiction. It also confirms one unspoken assumption of the study, which is that vocabulary comprehension strongly correlates to overall comprehension of a text. The conclusion posits that there are three types of reading for language acquisition, Intensive reading, extensive reading for language growth (target 98 percent words known), and extensive reading for developing fluency (target all words known).
%
\subsection{\fullcite{nation1992vocabulary}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
I found this paper in the literature review from \cite{hu_2000} and it felt relevant to explore more deeply.

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
This paper attempts to answer the question of if knowing 2,000 words is sufficient to enable reading young adult fiction for pleasure? Assuming the learner knows all 2,000 of the most common English words (as defined by West 1953), and the target 98 percent known words established by the paper above, the answer is no. It would take 5,000 words to reach sufficient coverage in most of the texts analyzed. The study then offers that the text can either be simplified to achieve this coverage, or the necessary vocabulary can actively studied before or during the reading process. Simplification is proposed at 2,500, 5,000 and 7,000 grades, and simplification rules are presumably enabled via the Oxford Concordance Programme.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
This study affirms that even young adult fiction requires too high a level of vocabulary for most second language learners. Thankfully, it provides evidence that these texts can be simplified to a level where a student who knows 2,500 words will have ninety-eight percent vocabulary coverage. It also begs the question of what the current standards are for programmatically simplifying texts.
%
\subsection{\fullcite{stajner-2021-automatic}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
The paper above made me eager to understand the current state of automatic text simplification, so I asked Chat GPT to generate a list of recent papers surveying the topic. This one seemed the most relevant.

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
Presents a six stage benchmark of adult literacy from the OECD, where the majority of native speakers never achieve the higher levels of reading comprehension described in the benchmark, pointing out that simplification is most useful for native speakers at the conceptual level, not syntactical or lexical. It discusses the roots of the field being driven by accessibility issues and early systems emphasizing rules-based approaches. It also points out two evolutions in the approaches to simplification, towards supervised machine learning around 2010, and again towards neural networks around 2015. Article points out that most simplification occurs at a sentence level, which can be misaligned with the common goal of simplifying an entire document or book under the same constraints. The author is bearish on the automatic evaluation of text simplification programs, but highlights SARI, SAMSA, and MT-based frameworks as the most promising.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
This article implies that modern system of automatic text simplification probably uses neural networks in some way. It also forced me to think about whether or not the scope of this project should include an evaluation framework, and if so, some of the challenges in choosing one. I had not previously considered using the project as a tool for learners with intellectual disabilities, but I now realize that may be an additional valid use case.

\subsection{\fullcite{xu-etal-2016-optimizing}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
This paper was cited by the paper above. I was eager to understand current approaches to evaluation frameworks.

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
The paper points out current limitations of machine-learning driven approaches to text simplification, noting problems in training data (notably the under-discussed limitations of simple English Wikipedia) and evaluation frameworks (notably their fragmentation across niche sub-disciplines). The paper emphasizes that an evaluation criteria is crucial since it serves as the objective function any machine learning algorithm should optimize. It then proposes SARI, \textbf{s}ystem output \textbf{a}gainst \textbf{r}eferences and against the \textbf{i}nput sentence. The paper uses both humans and SARI to evaluate its model's output, highlighting the moderate correlation between the two as evidence that evaluation can be automated. There is also a discussion of how SARI maps onto human evaluations of simplicity more effectively than BLEU, a framework designed for evaluating simplicity of translated texts.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
Biggest takeaway is actually the meta-observation on using quantitative human-generated data to prove the efficacy of some proposed computer-generated alternative. It is unlikely that I my project will allow for such scope, but is a fantastic idea. I also realize that evaluation frameworks for simplifying a text into the same target language (ie monolingual text simplification) may not be a well-studied field with consensus on the best tools to assess model accuracy. In the event that an LLM is insufficient at simplifying text, or generating new text at a sufficiently simple level, I may leverage the algorithm provided in this paper.
%
\subsection{\fullcite{sulem-etal-2018-semantic}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
This paper was also cited in (\cite{stajner-2021-automatic}).)

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
This paper proposes a novel framework for evaluating text simplification, referred to as SAMSA (\textbf{S}implification \textbf{A}uto-
matic evaluation \textbf{M}easure through \textbf{S}emantic \textbf{A}n-
notation). It continues with a review of the most commonly cited competing frameworks in the field, discussing the limitations of each. It explains the benefits of SAMSA, notably that it priotizes breaking complex sentences down into a higher quantity of more easily digestible sentences, ie "I read the book that John wrote" -> "John wrote a book. I read that book." The paper provides human input to validate that SAMSA's score strongly correlates to how humans rate text simplicity.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
This paper caused me to reconsider the goal of my project. The lack of consensus on evaluation frameworks for text simplification–not to mention the fact that an LLM may be able to provide a black box evaluation, of which there is very little research–makes a case for focusing on research into these benchmarks rather than a web application that has little concern for empirical assessment of its output.

%
\subsection{\fullcite{zhang2017sentence}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
Found this paper by Googling "Deep Learning Approaches to Sentence Simplification."

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
The paper proposes DRESS (\textbf{D}eep \textbf{Re}inforcement \textbf{S}entence \textbf{S}implification) that uses a decoder-encoder pattern in recurrent neural networks to simplify sentences. It uses SARI (\cite{xu-etal-2016-optimizing}) as the objective function but notes some of SARI's limitations. It runs DRESS and other models on three datasets, and evaluates them with multiple frameworks and dimensions. It concludes deeming DRESS a success, although there are many caveats from the high dimensionality of their evaluation.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
This paper provides strong evidence that neural networks are probably the most effective means of non-human sentence simplification available in 2024. Although whether an specialized LLM can outperform, much less compete on par with, a purpose-built neural network is impossible to conclude from this article alone.

\subsection{\fullcite{feng2023sentence}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
I asked ChatGPT to provide a list of papers, if any, that have recently compared LLM performance at sentence simplification against bespoke solutions like DRESS. 

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
This paper argues that LLMs are more effective than existing method of sentence simplification. It discusses differences in performance between zero- and few-shot prompting, across English, Spanish, and Portuguese. The study uses both automated metrics and those provided by three non-native English speakers to assess the efficacy of the LLM's output.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
This paper makes me feel more confident about building a project around the assumption that an LLM is a competent agent to outsource the process of sentence simplification. The article leaves plenty of follow-up questions, however, such as how the LLM's context window enables it to establish continuity in a story it continually generates, or how it might attune simplification to the vocabulary of a specific individual.

\subsection{\fullcite{kew2023bless}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
This was the other paper shortlisted by ChatGPT that examines the performance of an LLM at sentence simplification.

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
The paper is a more robust expansion of the one above, examining the performance of forty-four LLMs on three data sets of complex-simple sentence pairs. It distinguishes between open- and closed-weight (rather than -source) models and uses three prompts of varying verbosity. Using SARI and BERT as automated evaluation frameworks, the results found that more verbose instructions achieved higher-scoring output, and that OpenAI's (at the time) state-of-the-art GPT-3.5-Turbo outperformed all other LLMs by a significant margin. Although the paper also finds that humans are able to outperform all LLMs by a significant margin.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
This paper provides further evidence that LLMs are the best automated method of simplifying text, but also found that humans are still–at least at the time of writing–significantly better. Given that, to my knowledge, all graded readers are produced by humans, this begs the question as to whether LLMs are yet good enough at sentence simplification to replace the rote process of human participation.

\subsection{\fullcite{wu2024indepth}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
I asked ChatGPT to provide more recent papers, if available, comparing human to LLM performance.

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
The paper examines the performance of GPT 4 on sentence simplification. It introduces a novel approach to leverage humans to evaluate output, prompting humans to specifically call out errors in the hope of reducing variability and subjectivity of evaluation across different graders. Analysis of the human grading proved a low variability, with human grading corroborating automated grading that GPT 4 is the highest performing LLM to date. The paper calls out the model's weakness at lexical paraphrasing.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
This paper, despite dedicating a lot of discussion to optimizing humans as \textit{evaluators} of LLM output, doesn't make a conclusion on how well humans \textit{produce} sentencen simplifications compared to GPT-4. I've also noticed that most of the papers highlighted here so far rely on frighteningly few text corpora, notably Wiki{simple | large} and Newsela. This has me concerned that too many conclusions have been drawn from models all built on the same data. This may be the most thorough paper of the bunch so far.

\subsection{\fullcite{Murre2015ReplicationAA}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
I googled the best paper to cite in providing empirical evidence for spaced repetition.

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
This study replicates the famous "forgetting curve" popularized by Ebbinghaus in the 1880's and upon which spaced repetition is based. While the paper expands upon a phenomena at the twenty-four mark that does not fit the curve, they conclude that memory does follow a pseudo-logarithm (with refinement over the years) close to what Ebbinghaus initially proposed.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
Given that I had never read a paper on spaced repetition, I wanted to validate that it wasn't pop science but a genuine phenomenon that can be replicated in a research setting. This paper not only proves that, but also provides an equation that can easily be integrated into a computer program to keep track of when to best present recently acquired vocabulary.

\subsection{\fullcite{bullaughey2019reading}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
Was Googling whether any products exist that combine graded readers with spaced repetition.

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
This article presupposes that reading and spaced repetition are viewed as distinct tasks from one another, which builds an argument that it could be novel and beneficial to create a solution that is able to incorporate both. It outlines how most reading contains elements of spaced repetition by virtue of the same words being used repeatedly throughout. It argues that a balance of reading for pleasure and intentional flash card review is the ideal for a student of Mandarin Chinese.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
The fact that reading and SRS are presented as a dichotomy is encouraging evidence that an LLM, which can produce the next chapter of a story on-demand, may be able to tailor a reading experience to create a "just-in-time" approach to new vocabulary presentation. The article mentions the game WordSwing, which lets the participant self-evaluate the ideal level of vocabulary they want to see in the text. This may provide inspiration for ideas when making the web app.

\subsection{\fullcite{krashenreview}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
Returned to the linguistic arguments that language is acquired primarily through input, where Google led me back to Krashen.

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
Walks through Krashen's most well-known hypotheses and the criticisms surrounding them. These hypotheses include a distinction between language learning and acquisition, a prescribed order for learning different grammatical structures, the necessity of "i + 1" comprehensible input is necessary for successful acquisition, and that motivation is a key factor in learning. Most criticism stems from a lack of firm, testable definitions of these hypotheses, and the fact that they diminish the importance of output (speaking, writing).

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
I'm hesitant to let a hypothesis without data sway the design elements of this project. However, Krashen's hypotheses do align with some implicit assumptions of extensive reading, namely that passive acquisition of vocabulary through comprehensible input is a first-class method of mastering a second language. So it creates a springboard to explore papers that attempt to evaluate this claim empirically.

\subsection{\fullcite{karasimos2022battle}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
I searched Google Scholar for LingQ, the current language learning application I believe is the closest to my project.

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
Lots of throat clearing in this paper, which starts with a survey of the five applications chosen for the study and the rise of mobile assisted language learning (MALL). The study did not have access to the results of the participants' week-long experience with the apps, only a survey where they self-evaluate their preferences. Rosetta Stone and Duolingo had the highest overall scores across several categories, including, reading, speaking, writing, and listening.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
Biggest takeaway from this study is that not all studies are created equal. It is a nice survey of the most popular language learning apps but the second-hand nature of participant survey data and the general high-level of the questions creates little opportunity for grounded conclusions. The struggles of this paper highlight the challenge of comparing the efficacy of different consumer apps in a valid research setting. 

\subsection{\fullcite{Liu2015AnAO}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
This was another result returned by the same Google Scholar search above. I felt that understanding the use of social networks on language learning would be useful.

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
Article begins with a survey of different approaches to language learning afforded by the advent of Web 2.0, such as MOOCs and SNSLLs (\textbf{S}ocial \textbf{N}etwork \textbf{S}ites for \textbf{L}anguage \textbf{L}earning). The study chooses to examine Lang-8, LingQ, Italki, and Polyglotclub. Much of the results are a comparison of different features across the four apps, more or less a usability study. Lang-8 outperformed the other three sites across all benchmarks, as rated by language instructors.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
The study identifies Lang-8 (now HiNative) as the best-in-class social media platform for language learning. It does not make any conclusions about the efficacy of having input correction vs. extensive reading, although it can be assumed that both are necessary for successful \textit{adult} language learning, despite the optimal blend being unclear to me. I lack the networking expertise necessary to create such a platform, making it unlikely that my project for this course will include any social media components.

\subsection{\fullcite{Chang2015ImprovingRR}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
Asked Chat GPT for papers that explored whether listening to a transcription of a text while simultaneously reading it improved learning outcomes compared to silent reading.

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
Sixty-four students were engaged in a six month study where the control engaged in silent reading while the experimental group read the same texts but with accompanying audio. The paper begins with a literature review that points out limitations of previous work, notably that most studies only provide coarse measures of how much content was read (number of pages) as opposed to more quantifiable metrics (number of words), that many studies fail to measure both reading speed and comprehension, and that few attempt to answer the question as to how vocabulary acquisition through extensive reading interacts with the forgetting curve. Found that reading speed and comprehension were both higher in delayed post-test for the group that engaged in audio-assisted reading. Highlight paper.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
This paper makes a case for expanding the scope of the project to include audio to enable audio-assisted reading. Or even that enabling audio-assisted reading is more important than dynamically adjusting the text to optimize for spaced-repetition-optimized presentations of newly-encountered words. Although the premise is less novel than changing the subsequent parts of the story to attune to the needs of the reader.

\subsection{\fullcite{Nation2020GradedRA}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
Cited in the introduction of the paper above.

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
This paper makes important observations and lays important groundwork for the interaction between graded readers and extensive reading. The paper works backward from the observation that a word should be encountered ten times to guarantee cementing in long-term memory to suggest that seven graded readers are sufficient at each level of 500 additional new vocabulary words to achieve the right balance between scaffolding and learning. It also points out that many graded reading schemes in English only go up to 2,500 words, which is an insufficient vocabulary to begin extended reading of texts aimed at native speakers. The implication is that many graded reading schemes must be expanded to bring a student all the way from novice to capable of engaging with authentic content in the target language.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
This paper has me convinced that there is still room to improve most graded reading systems that enable extensive reading, validating the premise of the project. It also provides guidelines around how long to stay at each level that may be useful to inform an LLM, although a dynamic system may also make step changes in text difficulty unnecessary.

\subsection{\fullcite{TheChairmansBao_AI_2023}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
I use the Chairman's Bao and was curious if they had blog posts on LLM use for language learning.

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
The article provides an overview of current LLM-powered chatbots centered around learning a foreign language. It covers both vanilla models (Gemini, Chat-GPT) and apps tailored specifically for language learning on top of LLMs. It highlights several, including a common limitation of robotic-sounding voices and a default to text-based interactions, which diminish the value proposition of developing speaking and listening skills. It notes that leveraging LLMs for language learning is in its early days.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
On the one hand, this app review proves that LLM-powered chatbots for learning languages is already a saturated market. However, the fact that none of the ten apps highlighted focus on improving extensive reading content is somewhat reassuring, implying that this research project may unearth unexplored territory.

\subsection{\fullcite{mcdonald2016}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
Asked Chat-GPT for resources on whether looking up new words during extensive reading is preferable to purely passive vocabulary acquisition.

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
The papers introduces definitions of active vs passive learning, which I found more intuitive to re-label effortful vs effortless learning. To analogize vocabulary acquisition, I interpret active to be attempting to glean the definition of a word solely from context, and passive to have access to a dictionary for looking up new words. The study found that the most successful learners of certain boundary discrimination tasks were "passive-first", meaning that they were given examples initially but expected to form their own definitions later on.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
I read this paper to gather intuition for whether providing a dictionary for the words in the app is essential, optional, or even detrimental to learning outcomes. The safest conclusion is no conclusion at all–that the outcome of the tasks in the experiment are highly context-dependent and can't be assumed to transfer to language acquisition. The farther-reaching conclusion is that students should have access to a dictionary to look up the definition of a new word at initial exposures, potentially discouraging subsequent look-ups as they entrench the word into long-term memory.

\subsection{\fullcite{SuperMemo_Method_2023}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
Read the GitHub page for Anki software to discover it's based on Wozniak's Supermemo, wanted to understand its principles.

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
Introduces an elaboration on Ebbinghaus' Forgetting Curve, where a learner must self-select if a piece of content, is known, almost known, or forgotten. This selection dictates how long the system will wait until the next presentation of that content for recall. It notes the fact that Ebbinghaus' study tested memorization of useless information, making its relevance to real-world memory tasks suspect. It then describes Piotr Wozniak's Master's Thesis where he recreated a more robust, relevant version of the forgetting curve that would inform how Anki's spaced repetition works.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
It had not dawned on me that Anki requires the user to self-select how well they remembered something. This begs the question that if an extensive reading system over-represents vocabulary that has been looked up by the reader, implying it was forgotten, there's ambiguity in whether that lookup maps onto a complete forgetting or a "lesser" forgetting. Or if the UI needs to prompt the user to make that distinction.

\subsection{\fullcite{essay89410}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
Super Memo article mentions Leitner as seminal contributor to spaced repetition, wanted to understand how (if at all) the method had been challenged in recent years.

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
The paper attempts to model the memory state of a learner with a Markov Model. It presents interesting results from running a simulation of the model in Python, such as the estimation that 3.8 out of every five words studied will be mastered by the end of a two month period. The paper suggests changes to the Leitner system, such as adding an extra step in the progression from introduction to mastery, and intentional staggering of presentation of words to have fewer spikes in study time on certain days. The paper notes the need for validation via testing actual students.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
This 2022 paper implies that there is still ample room to optimize the spaced repetition algorithms underneath systems like Anki, regardless of whether the suggestions proposed in this specific paper are the most fruitful. It also highlights the complexity of maintaining the states of all the different words being studied, making an argument for leaning towards an API or other service that can abstract this logic away from the project.

\subsection{\fullcite{shortestpathrepetitionscheduling}}
%The paper’s bibliographic information (its APA citation, typically)
\subsubsection{Identification}
%In around one sentence, how you found it (a Google Scholar search? From a conference’s proceedings? From another paper’s references? %Something else?)
Reading the GitHub commit notes for Anki revealed that the newest version of their scheduling algorithm is based on this paper.

\subsubsection{Summary}
%In around three sentences, a brief, original summary in your own words
The paper highlights that many popular systems (including, at the time, Anki) continue to use rules-based approaches to spaced repetition, which at best have not been revisited in decades, and at worst generate inefficiencies in study time. Leveraging a dataset from MaiMemo, they generate the Difficulty-Halflife-P(recall) model, highlighting the weaknesses of a similar dataset provided by Duolingo. With this model simulating the memory of a learner, a stochastic shortest path algorithm is introduced to optimize the intervals between recall of the same content. Based on automated benchmarks (not human trials), it claims a 12.5 percent decrease in time to mastery over the (then current) Anki approach.

\subsubsection{Takeaways}
%In around three sentences, the main takeaways going forward
Not only is this further evidence that spaced repetition remains an evolving field, but it highlights the need for studies that replicate the findings of these optimization algorithms with human trials. The 12.5 percent improvement is impressive but no so great a decrease in memorization time to lose sleep over. It will be more important to use a service that abstracts handling of presentation intervals than use the most optimal methodology.

\section{Synthesis}
%In about a page, summarize the overall body of work you’ve put together. What are the high-level trends, large takeaways, or open questions you’ve found? If you’ve narrowed in on a particular domain, summarize that domain; if you’re still exploring, discuss the overall direction these efforts are leading you toward. Most importantly, anchor this synthesis in the papers you provided above, citing them where appropriate.

It's easiest to divide the above resources into a four distinct lines of inquiry:

\begin{enumerate}
    \item \textbf{Sentence Simplification.} How have approaches to sentence simplification changed over the years? What is the current state-of-the-art approach?
    \item \textbf{Extensive Reading.} Can we formally define Extensive Reading as a part of foreign language acquisition, if so, what is that definition, and is ER considered effective?
    \item \textbf{Spaced Repetition.} Is spaced repetition grounded in empirical evidence is there consensus on how to best implement it? If not, what are competing theories?
    \item \textbf{Miscellaneous.} A few miscellaneous papers on the role of audio-assisted reading (\cite{Chang2015ImprovingRR}), social media (\cite{Liu2015AnAO}), and consumer apps in language learning (\cite{karasimos2022battle}).
\end{enumerate}

\subsection{Trends}

For one, it's clear that \textbf{Sentence Simplification} has followed the same arc as most other NLP tasks in the last ten years. The field emerged decades ago, originating with rules-based approaches, for neural networks to become in vogue in the mid 2010's (\cite{xu-etal-2016-optimizing}, \cite{stajner-2021-automatic}, \cite{zhang2017sentence}), only for general purpose LLMs to become the best-in-class option in the last two years or so (\cite{feng2023sentence}). The papers also underscore the challenging in evaluating the output of a such a model, with frameworks like SAMSA (\cite{sulem-etal-2018-semantic}) and SARI being widely used (\cite{kew2023bless}) despite no consensus they can compete with human evaluators (\cite{wu2024indepth}). The take away from this observation is reassuring, since I was originally daunted by the potential need to implement my own solution to sentence simplification. These papers make leveraging the OpenAI API seem promising. This would allow for a lot more flexibility in the scope of the project.

Another observation is that \textbf{Extensive Reading} remains an accepted part of the language learning process. While the origin of the technique stems from Krashen's work (\cite{krashen_2004}), the growing skepticism over some of his hypotheses (\cite{krashenreview}) have been assuaged by empirical research that proved vocabulary acquisition is possible through extensive reading. The research explored here mostly supports ER being used in conjunction with more active learning techniques (\cite{mcdonald2016}). There's also empirical evidence that ninety-eight percent of the words in a text should be understood by the reader in order to enable passive acquisition of unknown vocabulary (\cite{hu_2000}) (\cite{nation1992vocabulary}). Other papers explore how audio narration of the written content can improve learning outcomes as well. There are also limitations in the availability of graded readers that enable extensive reading, as well as a potential pedagogical mismatch between different official milestones in language learning and providing a comprehensive scaffold to graduate to native content (\cite{Nation2020GradedRA}).

Finally, it's been reassuring that the basis of \textbf{Spaced Repetition} has been replicated (\cite{Murre2015ReplicationAA}) and improved (\cite{SuperMemo_Method_2023}, \cite{essay89410}, \cite{shortestpathrepetitionscheduling}) since Ebbinghaus' initial observation in the late 1800s. 

While each of these topics have been well-studied on their own, I did not find many papers that explored intersections between them. One article analogizes extensive reading and repetition (\cite{bullaughey2019reading}), but I did not find any papers that attempted to weave spaced repetition into a dynamically generated graded reader.

This observation has me feeling positive about my question exploring previously under-examined ground. That question being: Can an LLM dynamically adjust the difficulty of a text to create extensive reading content tailor-made for that learner's level?

\section{Reflection}
%In about half a page, reflect on the process of finding sources, reading papers, synthesizing their contents, and building your understanding. What was difficult, and what was easy? What are you finding yourself interested in going forward?
In general, the sheer volume of reading and writing would have been daunting if not for a deep personal interest in the topics covered by each paper. Configuring bibtex took longer than I had expected. I was also impressed by Chat GPT as a tool for identifying relevant papers, although it would frequently miss key pieces of nuances when suggesting some papers. I found it in general a superior launching pad than Google Scholar but a worse resource than a relevant paper's bibliography.

In retrospect, I may have spent too much time understanding the history of a topic rather than jumping to the best-in-breed current approach. However, I found it difficult to ask the right questions to unveil the more recent papers without finding seminal papers from the past, seeing which contemporary papers cited them, and reading about which new hypotheses they form.

I found it challenging to summarize papers that introduce entirely new frameworks, especially when they rely on math. It was liberating to keep the summaries to three sentences, but it forces glossing over any interesting statistical innovation in the paper.

I continue to be interested in my original project idea, of creating massive comprehensible input for extensive reading, with vocabulary in the text adjusting to reader's level.

\section{Planning}
%In about half a page, provide a plan for what you expect to do next week. What threads or ideas will you pursue? What questions will you seek answers to in the literature?
Some topics I'm interested in exploring next week:
\begin{enumerate}
    \item Above all, I'm interested in understanding the history of research into dynamic text generation, where a story's plot, complexity of language, or anything else responds to user input. This may begin in somewhat irrelevant territory of rules-based text adventure games, but with the intention of gleaning which frameworks and for what purposes such systems are created today.
    \item While I'm interested in creating a system for students of Mandarin Chinese, I'm aware that there are no spaces between words, meaning that a system will not be able to automatically identify which sequence of characters represent the word that a user means to click on (to signal they forgot it) without additional metadata or computation at time of lookup. So I will want to better understand how difficult it will be to identify word boundaries in Chinese, and consider pivoting to Spanish if too onerous.
    \item I want to research different web frameworks and dictionaries to use for my project and it will be interesting to see how many of these are covered by research vs. technical documentation. 
    \item This week's papers imply that there is a floor of approximately 2,500 words, at least in English, that a student must have in order to be able to read a text of reasonable complexity with ninety-eight percent comprehension. However, I have held a lower vocabulary in foreign languages and been able to find adequate graded readers. So I may look into this empirically
\end{enumerate}

\printbibliography{}

\end{document}

