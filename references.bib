@misc{merriamWebsterAPI,
  author = {Merriam-Webster},
  title = {Merriam-Webster Spanish-English Dictionary API Documentation},
  year = {2024},
  url = {https://dictionaryapi.com/products/api-spanish-dictionary},
  note = {Accessed: 2024-05-24}
}


@misc{mochiAPI,
  author = {Mochi},
  title = {Mochi API Documentation},
  year = {2024},
  url = {https://mochi.cards/docs/api/},
  note = {Accessed: 2024-05-24}
}

@article{Alahmadi2020ExploringTE,
  title={Exploring the effect of lexical inferencing and dictionary consultation on undergraduate EFL students’ vocabulary acquisition},
  author={Alaa Alahmadi and Anouschka Foltz},
  journal={PLoS ONE},
  year={2020},
  volume={15},
  url={https://api.semanticscholar.org/CorpusID:220889377}
}

@article{bilingual_dictionary,
author = {Laufer, Batia and Hadar, Linor},
year = {1997},
month = {06},
pages = {},
title = {Assessing the Effectiveness of Monolingual, Bilingual, and “Bilingualised” Dictionaries in the Comprehension and Production of New Words},
volume = {81},
journal = {The Modern Language Journal},
doi = {10.1111/j.1540-4781.1997.tb01174.x},
url= {https://www.researchgate.net/publication/262907003_Assessing_the_Effectiveness_of_Monolingual_Bilingual_and_Bilingualised_Dictionaries_in_the_Comprehension_and_Production_of_New_Words}
}

@article{dictionaryvalue,
  title={Dictionary Use While Reading: The Effects On Comprehension and Vocabulary Acquisition For Students Of Different Verbal Abilities},
  author={Susan Knight},
  journal={The Modern Language Journal},
  year={1994},
  volume={78},
  pages={285-299},
  url={https://www.jstor.org/stable/330108}
}

@book{dictionarypitfalls,
author = {Nesi, Hilary},
year = {2000},
month = {02},
pages = {},
title = {The Use and Abuse of EFL Dictionaries},
isbn = {3-484-30998-9}, 
url = {https://cronfa.swan.ac.uk/Record/cronfa42801/Download/0042801-02082018162522.pdf}
}

@article{vocabcorrelatesskill,
author = {Laufer, Batia and Nation, Paul},
year = {1995},
month = {09},
pages = {307-322},
title = {Vocabulary Size and Use: Lexical Richness in L2 Written Production},
volume = {16},
journal = {Applied Linguistics - APPL LINGUIST},
doi = {10.1093/applin/16.3.307},
url = {https://www.researchgate.net/publication/249237693_Vocabulary_Size_and_Use_Lexical_Richness_in_L2_Written_Production}
}

@article{imagesdictionary,
title = {Using the keyword method in the classroom: Is the interacting imagery necessary?},
journal = {System},
volume = {45},
pages = {17-26},
year = {2014},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2014.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X14000712},
author = {Dacian D. Dolean},
keywords = {Keyword method, Foreign language, Vocabulary learning, Imagery, Mnemonics},
abstract = {The Keyword Method (KWM) is one of the most extensively researched methods used in foreign language vocabulary learning. However, evidence for its successful classroom application is limited and research results are mixed. This paper reports two experiments using a simple teacher-friendly procedure in which assessment of the KWM on Romanian speaking subjects was used for the first time. Results indicate that showing simultaneously a picture for a new L2 vocabulary word and one for the keyword increases retention, even when pictures show no interaction of keyword and target word meanings when applied in elementary (studies 1 and 2) and middle school classrooms (study 2). The outcome of this research may benefit teachers and support immediate and long-term efficiency of the KWM.}
}

@article{clockworkorange,
title = {Vocabulary learning and reading},
journal = {System},
volume = {6},
number = {2},
pages = {72-78},
year = {1978},
issn = {0346-251X},
doi = {https://doi.org/10.1016/0346-251X(78)90027-1},
url = {https://www.wgtn.ac.nz/lals/resources/paul-nations-resources/paul-nations-publications/publications/documents/1978-Saragi-Clockwork-orange.pdf},
author = {T. Saragi and I.S.P. Nation and G.F. Meister}
}

@InProceedings{tooltipdictionary,
author    = {Guruprakash, J. and Krithika, L.B.},
title     = {TOOLTIP TRANSLATOR FOR ELEARNING COURSEWARE TO IMPROVE LEARNER’S VOCABULARY},
series    = {4th International Conference on Education and New Learning Technologies},
booktitle = {EDULEARN12 Proceedings},
isbn      = {978-84-695-3491-5},
issn      = {2340-1117},
publisher = {IATED},
location  = {Barcelona, Spain},
month     = {2-4 July, 2012},
year      = {2012},
pages     = {1406-1413}}

@article{dictionaryimportance,
author = {Luppescu, Stuart and Day, Richard R.},
title = {Reading, Dictionaries, and Vocabulary Learning},
journal = {Language Learning},
volume = {43},
number = {2},
pages = {263-279},
doi = {https://doi.org/10.1111/j.1467-1770.1992.tb00717.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-1770.1992.tb00717.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-1770.1992.tb00717.x},
abstract = {This article focuses on the contribution to vocabulary learning of the use of bilingual dictionaries during reading by 293 Japanese university students studying English as a foreign language. The results of the study show that students who used a dictionary scored significantly better on a vocabulary test than students who did not use a dictionary. However, evidence appeared for differential item functioning: Some items were harder for the group that used dictionaries. A possible explanation for this tendency is that students who were unable to locate the appropriate gloss in the dictionary were misled as to the meaning of the word. Moreover, students who used a dictionary read nearly half as quickly as the group that did not use dictionaries.},
year = {1993}
}

@article{ghimire2019comparative,
  title={Comparative Study on Python Web Frameworks: Flask and Django},
  author={Ghimire, Devndra},
  journal={International Journal of Scientific Research in Computer Science, Engineering and Information Technology},
  volume={5},
  number={2},
  pages={317-323},
  year={2019},
  publisher={IJSRCSEIT}
}


@article{chineseboundary,
author = {Gao, Jianfeng and Li, Mu and Huang, Changning and Wu, Andi},
year = {2005},
month = {12},
pages = {531-574},
title = {Chinese Word Segmentation and Named Entity Recognition: A Pragmatic Approach},
volume = {31},
journal = {Computational Linguistics},
doi = {10.1162/089120105775299177}
}

@article{cepedasrs,
author = {Nicholas J. Cepeda and Edward Vul and Doug Rohrer and John T. Wixted and Harold Pashler},
title ={Spacing Effects in Learning: A Temporal Ridgeline of Optimal Retention},

journal = {Psychological Science},
volume = {19},
number = {11},
pages = {1095-1102},
year = {2008},
doi = {10.1111/j.1467-9280.2008.02209.x},
    note ={PMID: 19076480},

URL = { 
    
        https://doi.org/10.1111/j.1467-9280.2008.02209.x
    
    

},
eprint = { 
    
        https://doi.org/10.1111/j.1467-9280.2008.02209.x
    
    

}
,
    abstract = { To achieve enduring retention, people must usually study information on multiple occasions. How does the timing of study events affect retention? Prior research has examined this issue only in a spotty fashion, usually with very short time intervals. In a study aimed at characterizing spacing effects over significant durations, more than 1,350 individuals were taught a set of facts and—after a gap of up to 3.5 months—given a review. A final test was administered at a further delay of up to 1 year. At any given test delay, an increase in the interstudy gap at first increased, and then gradually reduced, final test performance. The optimal gap increased as test delay increased. However, when measured as a proportion of test delay, the optimal gap declined from about 20 to 40\% of a 1-week test delay to about 5 to 10\% of a 1-year test delay. The interaction of gap and test delay implies that many educational practices are highly inefficient. }
}


@article{kangsrs,
author = {Kang, Sean},
year = {2016},
month = {01},
pages = {},
title = {Spaced Repetition Promotes Efficient and Effective Learning: Policy Implications for Instruction},
volume = {3},
journal = {Policy Insights from the Behavioral and Brain Sciences},
doi = {10.1177/2372732215624708}
}

@misc{mlq2023tokens,
  title={Tokens, Context Windows \& Large Language Models (LLMs)},
  author={{MLQ.ai}},
  year={2023},
  url={https://www.mlq.ai/tokens-context-window-llms/}
}

@misc{liu2023lost,
      title={Lost in the Middle: How Language Models Use Long Contexts}, 
      author={Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
      year={2023},
      eprint={2307.03172},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{pilán2016readable,
      title={A Readable Read: Automatic Assessment of Language Learning Materials based on Linguistic Complexity}, 
      author={Ildikó Pilán and Sowmya Vajjala and Elena Volodina},
      year={2016},
      eprint={1603.08868},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{caines2023application,
      title={On the application of Large Language Models for language teaching and assessment technology}, 
      author={Andrew Caines and Luca Benedetto and Shiva Taslimipoor and Christopher Davis and Yuan Gao and Oeistein Andersen and Zheng Yuan and Mark Elliott and Russell Moore and Christopher Bryant and Marek Rei and Helen Yannakoudakis and Andrew Mullooly and Diane Nicholls and Paula Buttery},
      year={2023},
      eprint={2307.08393},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@book{
    Lave_Wenger_1991,
    place={Cambridge},
    series={Learning in Doing: Social, Cognitive and Computational Perspectives}, 
    title={Situated Learning: Legitimate Peripheral Participation},
    publisher={Cambridge University Press},
    author={Lave, Jean and Wenger, Etienne},
    year={1991},
    collection={Learning in Doing: Social, Cognitive and Computational Perspectives}
}

@article{krashen_2004,
author = {Krashen, Stephen},
year = {2004},
month = {01},
title = {The Power of Reading: Insights from the Research}
}

@article{hu_2000,
author = {Hu, Marcella and Nation, Paul},
year = {2000},
month = {01},
title = {Unknown Vocabulary Density and Reading Comprehension},
volume = {13},
journal = {Reading in a Foreign Language}
}

@article{nation1992vocabulary,
  title={What Vocabulary Size Is Needed to Read Unsimplified Texts for Pleasure?},
  author={Nation, Paul and Hirsch, David},
  journal={Reading in a Foreign Language},
  volume={8},
  number={2},
  pages={689--696},
  year={1992}
}
@inproceedings{stajner-2021-automatic,
    title = "Automatic Text Simplification for Social Good: Progress and Challenges",
    author = "Stajner, Sanja",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.233",
    doi = "10.18653/v1/2021.findings-acl.233",
    pages = "2637--2652",
}

@article{xu-etal-2016-optimizing,
    title = "Optimizing Statistical Machine Translation for Text Simplification",
    author = "Xu, Wei  and
      Napoles, Courtney  and
      Pavlick, Ellie  and
      Chen, Quanze  and
      Callison-Burch, Chris",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q16-1029",
    doi = "10.1162/tacl_a_00107",
    pages = "401--415",
    abstract = "Most recent sentence simplification systems use basic machine translation models to learn lexical and syntactic paraphrases from a manually simplified parallel corpus. These methods are limited by the quality and quantity of manually simplified corpora, which are expensive to build. In this paper, we conduct an in-depth adaptation of statistical machine translation to perform text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task.",
}

@inproceedings{sulem-etal-2018-semantic,
    title = "Semantic Structural Evaluation for Text Simplification",
    author = "Sulem, Elior  and
      Abend, Omri  and
      Rappoport, Ari",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1063",
    doi = "10.18653/v1/N18-1063",
    pages = "685--696",
    abstract = "Current measures for evaluating text simplification systems focus on evaluating lexical text aspects, neglecting its structural aspects. In this paper we propose the first measure to address structural aspects of text simplification, called SAMSA. It leverages recent advances in semantic parsing to assess simplification quality by decomposing the input based on its semantic structure and comparing it to the output. SAMSA provides a reference-less automatic evaluation procedure, avoiding the problems that reference-based methods face due to the vast space of valid simplifications for a given sentence. Our human evaluation experiments show both SAMSA{'}s substantial correlation with human judgments, as well as the deficiency of existing reference-based measures in evaluating structural simplification.",
}

@misc{zhang2017sentence,
      title={Sentence Simplification with Deep Reinforcement Learning}, 
      author={Xingxing Zhang and Mirella Lapata},
      year={2017},
      eprint={1703.10931},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{feng2023sentence,
      title={Sentence Simplification via Large Language Models}, 
      author={Yutao Feng and Jipeng Qiang and Yun Li and Yunhao Yuan and Yi Zhu},
      year={2023},
      eprint={2302.11957},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
} 

@misc{kew2023bless,
      title={BLESS: Benchmarking Large Language Models on Sentence Simplification}, 
      author={Tannon Kew and Alison Chi and Laura Vásquez-Rodríguez and Sweta Agrawal and Dennis Aumiller and Fernando Alva-Manchego and Matthew Shardlow},
      year={2023},
      eprint={2310.15773},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wu2024indepth,
      title={An In-depth Evaluation of GPT-4 in Sentence Simplification with Error-based Human Assessment}, 
      author={Xuanxin Wu and Yuki Arase},
      year={2024},
      eprint={2403.04963},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Murre2015ReplicationAA,
  title={Replication and Analysis of Ebbinghaus’ Forgetting Curve},
  author={Jaap M. J. Murre and Joeri Dros},
  journal={PLoS ONE},
  year={2015},
  volume={10},
  url={https://api.semanticscholar.org/CorpusID:9151801}
}

@article{bullaughey2019reading,
  title={Reading is a lot like spaced repetition, only better},
  author={Bullaughey, K.},
  year={2019},
  url={https://www.hackingchinese.com/reading-is-a-lot-like-spaced-repetition-only-better/},
  journal={Hacking Chinese},
  note={Retrieved from \url{https://www.hackingchinese.com/reading-is-a-lot}}
}

@article{krashenreview,
author = {Luo, Zixu},
year = {2024},
month = {03},
pages = {130-135},
title = {A Review of Krashen’s Input Theory},
volume = {26},
journal = {Journal of Education, Humanities and Social Sciences},
doi = {10.54097/3fnf5786}
}

@article{karasimos2022battle,
  title={The battle of language learning apps: a cross-platform overview},
  author={Karasimos, Athanasios},
  journal={Research Papers in Language Teaching and Learning},
  volume={12},
  number={1},
  pages={150--166},
  year={2022},
  publisher={Hellenic Open University}
}

@article{Liu2015AnAO,
  title={An Analysis of Social Network Websites for Language Learning: Implications for Teaching and Learning English as a Second Language},
  author={Min Liu and Kana Abe and Mengwen Cao and Sa Liu and Duygu Uslu Ok and Jeong-bin Hannah Park and Claire Meadows Parrish and Veronica Gabriela Sardegna},
  journal={the CALICO Journal},
  year={2015},
  volume={32},
  pages={114-152},
  url={https://api.semanticscholar.org/CorpusID:54206295}
}

@article{Chang2015ImprovingRR,
  title={Improving reading rates and comprehension through audio-assisted extensive reading for beginner learners},
  author={Anna C.-S. Chang and Sonia Millett},
  journal={System},
  year={2015},
  volume={52},
  pages={91-102},
  url={https://api.semanticscholar.org/CorpusID:60160153}
}

@article{Nation2020GradedRA,
  title={Graded Readers and Vocabulary},
  author={Paul Nation and Karen Wang Ming-Tzu},
  journal={Reading in a foreign language},
  year={2020},
  volume={12},
  url={https://api.semanticscholar.org/CorpusID:60539916}
}

@misc{TheChairmansBao_AI_2023,
    author = {{The Chairman's Bao}},
    title = {AI Apps to Help You Learn Chinese},
    year = {2023},
    url = {https://www.thechairmansbao.com/blog/ai-apps-learn-chinese/},
    note = {Accessed: 2024-05-19}
}

@inproceedings{mcdonald2016,
author = {Macdonald, Kyle and Frank, Michael},
year = {2016},
month = {08},
pages = {},
title = {When does passive learning improve the effectiveness of active learning?}
}

@misc{SuperMemo_Method_2023,
    author = {{SuperMemo World}},
    title = {The SuperMemo Method},
    year = {2023},
    url = {https://www.supermemo.com/en/supermemo-method},
    note = {Accessed: 2024-05-19}
}

@misc{essay89410,
            year = {2022},
           month = {February},
          author = {A.E. {Beursgens}},
           title = {A Markov process analysis of and a proposal for adjustments to the Leitner system},
             url = {http://essay.utwente.nl/89410/},
        abstract = {The Leitner system is a review scheme for flashcards that uses spaced repetition. Although there has been quite some research performed in ways to implement spaced repetition on computers, no existing simple mathematical model for the physical use of the Leitner system could be found. The Leitner system is modelled as a discrete-time inhomogeneous Markov process. The system is analysed using three performance measures, taking into account the efficiency of the system and the distribution of the workload. The relation between the number of words in the system and both the number of words mastered after as well as reviewed on day $t$ is reasoned to be linear. Moreover, the ratio between the number of words reviewed on two days of a cycle is shown to be fixed. Furthermore, the influence of the global difficulty on the results is examined. Several adjustments to the Leitner system are proposed to make the system more user friendly, both in equalizing the distribution of workload over days and in adjusting it to weekly cycles. Finally, the lessons learned from those adjustments are used to propose an alternative to the Leitner system. However, the influence of the adjustments to the system on the long-term retention of the studied words could not be examined and needs further research.}
}

@inproceedings{shortestpathrepetitionscheduling,
author = {Ye, Junyao and Su, Jingyong and Cao, Yilong},
title = {A Stochastic Shortest Path Algorithm for Optimizing Spaced Repetition Scheduling},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539081},
doi = {10.1145/3534678.3539081},
abstract = {Spaced repetition is a mnemonic technique where long-term memory can be efficiently formed by following review schedules. For greater memorization efficiency, spaced repetition schedulers need to model students' long-term memory and optimize the review cost. We have collected 220 million students' memory behavior logs with time-series features and built a memory model with Markov property. Based on the model, we design a spaced repetition scheduler guaranteed to minimize the review cost by a stochastic shortest path algorithm. Experimental results have shown a 12.6\% performance improvement over the state-of-the-art methods. The scheduler has been successfully deployed in the online language-learning app MaiMemo to help millions of students.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4381–4390},
numpages = {10},
keywords = {spaced repetition, optimal control, language learning},
location = {Washington DC, USA},
series = {KDD '22}
}
