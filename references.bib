
@book{
    Lave_Wenger_1991,
    place={Cambridge},
    series={Learning in Doing: Social, Cognitive and Computational Perspectives}, 
    title={Situated Learning: Legitimate Peripheral Participation},
    publisher={Cambridge University Press},
    author={Lave, Jean and Wenger, Etienne},
    year={1991},
    collection={Learning in Doing: Social, Cognitive and Computational Perspectives}
}

@article{krashen_2004,
author = {Krashen, Stephen},
year = {2004},
month = {01},
title = {The Power of Reading: Insights from the Research}
}

@article{hu_2000,
author = {Hu, Marcella and Nation, Paul},
year = {2000},
month = {01},
title = {Unknown Vocabulary Density and Reading Comprehension},
volume = {13},
journal = {Reading in a Foreign Language}
}

@article{nation1992vocabulary,
  title={What Vocabulary Size Is Needed to Read Unsimplified Texts for Pleasure?},
  author={Nation, Paul and Hirsch, David},
  journal={Reading in a Foreign Language},
  volume={8},
  number={2},
  pages={689--696},
  year={1992}
}
@inproceedings{stajner-2021-automatic,
    title = "Automatic Text Simplification for Social Good: Progress and Challenges",
    author = "Stajner, Sanja",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.233",
    doi = "10.18653/v1/2021.findings-acl.233",
    pages = "2637--2652",
}

@article{xu-etal-2016-optimizing,
    title = "Optimizing Statistical Machine Translation for Text Simplification",
    author = "Xu, Wei  and
      Napoles, Courtney  and
      Pavlick, Ellie  and
      Chen, Quanze  and
      Callison-Burch, Chris",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q16-1029",
    doi = "10.1162/tacl_a_00107",
    pages = "401--415",
    abstract = "Most recent sentence simplification systems use basic machine translation models to learn lexical and syntactic paraphrases from a manually simplified parallel corpus. These methods are limited by the quality and quantity of manually simplified corpora, which are expensive to build. In this paper, we conduct an in-depth adaptation of statistical machine translation to perform text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task.",
}

@inproceedings{sulem-etal-2018-semantic,
    title = "Semantic Structural Evaluation for Text Simplification",
    author = "Sulem, Elior  and
      Abend, Omri  and
      Rappoport, Ari",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1063",
    doi = "10.18653/v1/N18-1063",
    pages = "685--696",
    abstract = "Current measures for evaluating text simplification systems focus on evaluating lexical text aspects, neglecting its structural aspects. In this paper we propose the first measure to address structural aspects of text simplification, called SAMSA. It leverages recent advances in semantic parsing to assess simplification quality by decomposing the input based on its semantic structure and comparing it to the output. SAMSA provides a reference-less automatic evaluation procedure, avoiding the problems that reference-based methods face due to the vast space of valid simplifications for a given sentence. Our human evaluation experiments show both SAMSA{'}s substantial correlation with human judgments, as well as the deficiency of existing reference-based measures in evaluating structural simplification.",
}

@misc{zhang2017sentence,
      title={Sentence Simplification with Deep Reinforcement Learning}, 
      author={Xingxing Zhang and Mirella Lapata},
      year={2017},
      eprint={1703.10931},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{feng2023sentence,
      title={Sentence Simplification via Large Language Models}, 
      author={Yutao Feng and Jipeng Qiang and Yun Li and Yunhao Yuan and Yi Zhu},
      year={2023},
      eprint={2302.11957},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
} 

@misc{kew2023bless,
      title={BLESS: Benchmarking Large Language Models on Sentence Simplification}, 
      author={Tannon Kew and Alison Chi and Laura Vásquez-Rodríguez and Sweta Agrawal and Dennis Aumiller and Fernando Alva-Manchego and Matthew Shardlow},
      year={2023},
      eprint={2310.15773},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wu2024indepth,
      title={An In-depth Evaluation of GPT-4 in Sentence Simplification with Error-based Human Assessment}, 
      author={Xuanxin Wu and Yuki Arase},
      year={2024},
      eprint={2403.04963},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Murre2015ReplicationAA,
  title={Replication and Analysis of Ebbinghaus’ Forgetting Curve},
  author={Jaap M. J. Murre and Joeri Dros},
  journal={PLoS ONE},
  year={2015},
  volume={10},
  url={https://api.semanticscholar.org/CorpusID:9151801}
}

@article{bullaughey2019reading,
  title={Reading is a lot like spaced repetition, only better},
  author={Bullaughey, K.},
  year={2019},
  url={https://www.hackingchinese.com/reading-is-a-lot-like-spaced-repetition-only-better/},
  journal={Hacking Chinese},
  note={Retrieved from \url{https://www.hackingchinese.com/reading-is-a-lot}}
}

@article{krashenreview,
author = {Luo, Zixu},
year = {2024},
month = {03},
pages = {130-135},
title = {A Review of Krashen’s Input Theory},
volume = {26},
journal = {Journal of Education, Humanities and Social Sciences},
doi = {10.54097/3fnf5786}
}

@article{karasimos2022battle,
  title={The battle of language learning apps: a cross-platform overview},
  author={Karasimos, Athanasios},
  journal={Research Papers in Language Teaching and Learning},
  volume={12},
  number={1},
  pages={150--166},
  year={2022},
  publisher={Hellenic Open University}
}

@article{Liu2015AnAO,
  title={An Analysis of Social Network Websites for Language Learning: Implications for Teaching and Learning English as a Second Language},
  author={Min Liu and Kana Abe and Mengwen Cao and Sa Liu and Duygu Uslu Ok and Jeong-bin Hannah Park and Claire Meadows Parrish and Veronica Gabriela Sardegna},
  journal={the CALICO Journal},
  year={2015},
  volume={32},
  pages={114-152},
  url={https://api.semanticscholar.org/CorpusID:54206295}
}

@article{Chang2015ImprovingRR,
  title={Improving reading rates and comprehension through audio-assisted extensive reading for beginner learners},
  author={Anna C.-S. Chang and Sonia Millett},
  journal={System},
  year={2015},
  volume={52},
  pages={91-102},
  url={https://api.semanticscholar.org/CorpusID:60160153}
}

@article{Nation2020GradedRA,
  title={Graded Readers and Vocabulary},
  author={Paul Nation and Karen Wang Ming-Tzu},
  journal={Reading in a foreign language},
  year={2020},
  volume={12},
  url={https://api.semanticscholar.org/CorpusID:60539916}
}

@misc{TheChairmansBao_AI_2023,
    author = {{The Chairman's Bao}},
    title = {AI Apps to Help You Learn Chinese},
    year = {2023},
    url = {https://www.thechairmansbao.com/blog/ai-apps-learn-chinese/},
    note = {Accessed: 2024-05-19}
}

@inproceedings{mcdonald2016,
author = {Macdonald, Kyle and Frank, Michael},
year = {2016},
month = {08},
pages = {},
title = {When does passive learning improve the effectiveness of active learning?}
}

@misc{SuperMemo_Method_2023,
    author = {{SuperMemo World}},
    title = {The SuperMemo Method},
    year = {2023},
    url = {https://www.supermemo.com/en/supermemo-method},
    note = {Accessed: 2024-05-19}
}

@misc{essay89410,
            year = {2022},
           month = {February},
          author = {A.E. {Beursgens}},
           title = {A Markov process analysis of and a proposal for adjustments to the Leitner system},
             url = {http://essay.utwente.nl/89410/},
        abstract = {The Leitner system is a review scheme for flashcards that uses spaced repetition. Although there has been quite some research performed in ways to implement spaced repetition on computers, no existing simple mathematical model for the physical use of the Leitner system could be found. The Leitner system is modelled as a discrete-time inhomogeneous Markov process. The system is analysed using three performance measures, taking into account the efficiency of the system and the distribution of the workload. The relation between the number of words in the system and both the number of words mastered after as well as reviewed on day $t$ is reasoned to be linear. Moreover, the ratio between the number of words reviewed on two days of a cycle is shown to be fixed. Furthermore, the influence of the global difficulty on the results is examined. Several adjustments to the Leitner system are proposed to make the system more user friendly, both in equalizing the distribution of workload over days and in adjusting it to weekly cycles. Finally, the lessons learned from those adjustments are used to propose an alternative to the Leitner system. However, the influence of the adjustments to the system on the long-term retention of the studied words could not be examined and needs further research.}
}

@inproceedings{shortestpathrepetitionscheduling,
author = {Ye, Junyao and Su, Jingyong and Cao, Yilong},
title = {A Stochastic Shortest Path Algorithm for Optimizing Spaced Repetition Scheduling},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539081},
doi = {10.1145/3534678.3539081},
abstract = {Spaced repetition is a mnemonic technique where long-term memory can be efficiently formed by following review schedules. For greater memorization efficiency, spaced repetition schedulers need to model students' long-term memory and optimize the review cost. We have collected 220 million students' memory behavior logs with time-series features and built a memory model with Markov property. Based on the model, we design a spaced repetition scheduler guaranteed to minimize the review cost by a stochastic shortest path algorithm. Experimental results have shown a 12.6\% performance improvement over the state-of-the-art methods. The scheduler has been successfully deployed in the online language-learning app MaiMemo to help millions of students.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4381–4390},
numpages = {10},
keywords = {spaced repetition, optimal control, language learning},
location = {Washington DC, USA},
series = {KDD '22}
}
