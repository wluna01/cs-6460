@misc{generative_information_retrieval,
      title={From Matching to Generation: A Survey on Generative Information Retrieval}, 
      author={Xiaoxi Li and Jiajie Jin and Yujia Zhou and Yuyao Zhang and Peitian Zhang and Yutao Zhu and Zhicheng Dou},
      year={2024},
      eprint={2404.14851},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{flashcard_scheduler_evolution,
      title={KARL: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students}, 
      author={Matthew Shu and Nishant Balepur and Shi Feng and Jordan Boyd-Graber},
      year={2024},
      eprint={2402.12291},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{llm_augmented_exercise_retrieval,
author = {Xu, Austin and Monroe, Will and Bicknell, Klinton},
title = {Large language model augmented exercise retrieval for personalized language learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636883},
doi = {10.1145/3636555.3636883},
abstract = {We study the problem of zero-shot exercise retrieval in the context of online language learning, to give learners the ability to explicitly request personalized exercises via natural language. Using real-world data collected from language learners, we observe that vector similarity approaches poorly capture the relationship between exercise content and the language that learners use to express what they want to learn. This semantic gap between queries and content dramatically reduces the effectiveness of general-purpose retrieval models pretrained on large scale information retrieval datasets like MS MARCO&nbsp;[2]. We leverage the generative capabilities of large language models to bridge the gap by synthesizing hypothetical exercises based on the learner’s input, which are then used to search for relevant exercises. Our approach, which we call mHyER, overcomes three challenges: (1) lack of relevance labels for training, (2) unrestricted learner input content, and (3) low semantic similarity between input and retrieval candidates. mHyER outperforms several strong baselines on two novel benchmarks created from crowdsourced data and publicly available data.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {284–294},
numpages = {11},
keywords = {large language models, online language learning, personalization, zero-shot exercise retrieval},
location = {<conf-loc>, <city>Kyoto</city>, <country>Japan</country>, </conf-loc>},
series = {LAK '24}
}

@misc{question_generation_adaptive_education,
author = {Srivastava, Megha and Goodman, Noah},
year = {2021},
month = {06},
pages = {},
title = {Question Generation for Adaptive Education}
}

@misc{deep_knowledge_tracing,
      title={Deep Knowledge Tracing}, 
      author={Chris Piech and Jonathan Spencer and Jonathan Huang and Surya Ganguli and Mehran Sahami and Leonidas Guibas and Jascha Sohl-Dickstein},
      year={2015},
      eprint={1506.05908},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{important_adaptive_learning_exercise_generation,
      title={Adaptive and Personalized Exercise Generation for Online Language Learning}, 
      author={Peng Cui and Mrinmaya Sachan},
      year={2023},
      eprint={2306.02457},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{taiwan_adaptive_testing,
  title={Leveraging LLMs for Adaptive Testing and Learning in Taiwan Adaptive Learning Platform (TALP)},
  author={Bor-Chen Kuo and Frederic T. Y. Chang and Zong-En Bai},
  journal={National Taichung University of Education},
  address={140 Minsheng Rd, West Dist., Taichung City, 403514, Taiwan},
  year={2024},
    url = {https://ceur-ws.org/Vol-3487/paper6.pdf}
}


@misc{hyplern_interlinear_reading,
    author = {{Hyplern}},
    title = {Extensive Reading with Spaced Repetition},
    year = {2015},
    url = {https://shop.hyplern.com/blogs/news/18949315-extensive-reading-with-spaced-repetition},
    note = {Accessed: 2024-05-27}
}

@misc{ai_human_taking_turns_creating_story,
      title={Collaborative Storytelling with Large-scale Neural Language Models}, 
      author={Eric Nichols and Leo Gao and Randy Gomez},
      year={2020},
      eprint={2011.10208},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{recent_story_generation_review,
      title={The Next Chapter: A Study of Large Language Models in Storytelling}, 
      author={Zhuohan Xie and Trevor Cohn and Jey Han Lau},
      year={2023},
      eprint={2301.09790},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{controllable_story_generation,
    title = "Towards Controllable Story Generation",
    author = "Peng, Nanyun  and
      Ghazvininejad, Marjan  and
      May, Jonathan  and
      Knight, Kevin",
    editor = "Mitchell, Margaret  and
      Huang, Ting-Hao {`}Kenneth{'}  and
      Ferraro, Francis  and
      Misra, Ishan",
    booktitle = "Proceedings of the First Workshop on Storytelling",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-1505",
    doi = "10.18653/v1/W18-1505",
    pages = "43--49",
    abstract = "We present a general framework of analyzing existing story corpora to generate controllable and creative new stories. The proposed framework needs little manual annotation to achieve controllable story generation. It creates a new interface for humans to interact with computers to generate personalized stories. We apply the framework to build recurrent neural network (RNN)-based generation models to control story ending valence and storyline. Experiments show that our methods successfully achieve the control and enhance the coherence of stories through introducing storylines. with additional control factors, the generation model gets lower perplexity, and yields more coherent stories that are faithful to the control factors according to human evaluation.",
}


@article{vygostky_krashen_not_same,
author = {Dunn, William and Lantolf, James},
year = {2002},
month = {12},
pages = {411 - 442},
title = {Vygotsky's Zone of Proximal Development and Krashen's i+1: Incommensurable Constructs; Incommensurable Theories},
volume = {48},
journal = {Language Learning},
doi = {10.1111/0023-8333.00048}
}

@misc{llmtutor,
      title={Scaffolding Language Learning via Multi-modal Tutoring Systems with Pedagogical Instructions}, 
      author={Zhengyuan Liu and Stella Xin Yin and Carolyn Lee and Nancy F. Chen},
      year={2024},
      eprint={2404.03429},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inbook{vygotsky,
 ISBN = {9780674576285},
 URL = {http://www.jstor.org/stable/j.ctvjf9vz4.11},
 abstract = {The problems encountered in the psychological analysis of teaching cannot be correctly resolved or even formulated without addressing the relation between learning and development in school-age children. Yet it is the most unclear of all the basic issues on which the application of child development theories to educational processes depends. Needless to say, the lack of theoretical clarity does not mean that the issue is removed altogether from current research efforts into learning; not one study can avoid this central theoretical issue. But the relation between learning and development remains methodologically unclear because concrete research studies have embodied theoretically vague,},
 author = {L. S. VYGOTSKY},
 booktitle = {Mind in Society: Development of Higher Psychological Processes},
 pages = {79--91},
 publisher = {Harvard University Press},
 title = {Interaction between Learning and Development},
 urldate = {2024-05-27},
 year = {1978},
 url = {https://archive.org/details/levs.vygotskymindinsocietythedevelopmentzlib.org/page/n89/mode/2up}
}


@inproceedings{ye2023storypark,
  title={Storypark: Leveraging Large Language Models to Enhance Children Story Learning Through Child-AI Collaboration Storytelling},
  author={Ye, Lyumanshan and Jiang, Jiandong and Chang, Danni and Liu, Pengfei},
  booktitle={Proceedings of the 2023 Conference on Educational Technologies},
  year={2023},
  organization={Shanghai Jiaotong University},
  address={Shanghai, China},
  url={https://example.com/storypark2023}
}

@article{Fabriz2021,
  author    = {Fabriz, S. and Mendzheritskaya, J. and Stehle, S.},
  title     = {Impact of Synchronous and Asynchronous Settings of Online Teaching and Learning in Higher Education on Students’ Learning Experience During COVID-19},
  journal   = {Frontiers in Psychology},
  year      = {2021},
  volume    = {12},
  pages     = {733554},
  doi       = {10.3389/fpsyg.2021.733554},
  url       = {https://www.frontiersin.org/articles/10.3389/fpsyg.2021.733554/full},
}

@misc{AdvancedEducation2012,
  author       = {{Advanced Education}},
  title        = {{When is a MOOC Not a MOOC? What MOOC Means Today}},
  year         = 2012,
  url          = {https://advanceducation.blogspot.com/2012/11/when-is-mooc-not-mooc-what-mooc-means.html},
  note         = {Accessed: 2024-05-26}
}

@misc{hindustan,
  author       = {{Hindustan Times}},
  title        = {{MOOCs offered on Swayam will be accepted for credit mobility in colleges: MHRD}},
  year         = 2023,
  url          = {https://www.hindustantimes.com/education/moocs-offered-on-swayam-will-be-accepted-for-credit-mobility-in-colleges-mhrd/story-VF4gAzK6Wz0mPXuvmYkKgJ.html},
  note         = {Accessed: 2024-05-26}
}

@misc{GeorgiaTech2023,
  author       = {{Georgia Tech Research Institute}},
  title        = {{Georgia Tech Introduces New Computer Science Fellowship During Liberian Presidential Visit}},
  year         = 2023,
  url          = {https://research.gatech.edu/georgia-tech-introduces-new-computer-science-fellowship-during-liberian-presidential-visit},
  note         = {Accessed: 2024-05-26}
}

@misc{JoynerReddit2024,
  author       = {Professor David Joyner},
  title        = {Is the Drop Out Rate for OMSCS High Because of the Rigor of the Program?},
  year         = {2024},
  url          = {https://www.reddit.com/r/OMSCS/comments/sgg9ne/is_the_drop_out_rate_for_omscs_high_because_of/},
  note         = {Accessed: 2024-05-25}
}

@misc{Joyner2024,
  author       = {Professor David Joyner},
  title        = {Educational Forum Discussion},
  year         = {2024},
  note         = {Spoken at an educational forum on May 22, 2024}
}


@misc{OMSCSStats2021,
  title        = {OMSCS Program Statistics - Fall 2021},
  author       = {Georgia Institute of Technology},
  year         = {2021},
  url          = {https://omscs.gatech.edu/stats-fall-2021},
  note         = {Accessed: 2024-05-25}
}


@misc{GoodWillHunting1997,
  title        = {Good Will Hunting},
  author       = {Gus Van Sant, director},
  year         = {1997},
  note         = {Miramax Films},
  howpublished = {Film},
  quote        = {"You wasted $150,000 on an education you could have got for $1.50 in late charges at the public library."}
}


@misc{ForbesTuitionInflation2024,
  title        = {College Tuition Inflation: Compare The Cost Of College Over Time},
  author       = {Kaitlin Mulhere},
  year         = {2024},
  url          = {https://www.forbes.com/advisor/student-loans/college-tuition-inflation/},
  note         = {Accessed: 2024-05-25}
}


@misc{StanfordSolarPanels2022,
  title        = {Solar Panels Largely Confined to Wealthy Americans},
  author       = {Stanford News},
  year         = {2022},
  url          = {https://news.stanford.edu/stories/2022/11/solar-panels-largely-confined-wealthy-americans},
  note         = {Accessed: 2024-05-25}
}


@misc{StanfordRegistrarBulletin,
  title        = {Stanford University Registrar's Office Bulletin 2011-2012},
  author       = {Stanford University Registrar},
  year         = {2011},
  url          = {http://web.stanford.edu/dept/registrar/bulletin1112/4832.htm},
  note         = {Accessed: 2024-05-25}
}

@misc{MITOCWFAQs,
  title        = {MIT OpenCourseWare FAQs},
  author       = {MIT OpenCourseWare},
  year         = {2024},
  url          = {https://ocw.mit.edu/ans7870/global/MIT_OpenCourseWare_FAQs.pdf},
  note         = {Accessed: 2024-05-25}
}

@article{Lieberman2018,
  author       = {Mark Lieberman},
  title        = {In Quest for Long-Term Sustainability, edX Tries to Monetize MOOCs},
  journal      = {Inside Higher Ed},
  year         = {2018},
  url          = {https://www.insidehighered.com/digital-learning/article/2018/12/18/quest-long-term-sustainability-edx-tries-monetize-moocs},
  note         = {Accessed: 2024-05-25}
}

@book{freeschool,
  title        = {Education at a Glance 2022: OECD Indicators},
  author       = {Organisation for Economic Co-operation and Development},
  year         = {2022},
  publisher    = {OECD Publishing},
  address      = {Paris},
  doi          = {10.1787/69096873-en},
  url          = {https://www.oecd-ilibrary.org/education/education-at-a-glance-2022_eag-2022-en},
}

@misc{collegeboard,
  author = {{College Board}},
  title = {Trends in College Pricing},
  year = {2020},
  url = {https://trends.collegeboard.org/college-pricing}
}

@article{ford,
  title={How Henry Ford’s Model T Changed The World},
  author={Petersen, Randy},
  journal={Smithsonian Magazine},
  year={2014},
  url={https://www.smithsonianmag.com/innovation/how-henry-fords-model-t-changed-the-world-20140277/}
}

@article{Thrun2012,
  author = {Sebastian Thrun},
  title = {Sebastian Thrun Will Teach You How to Build Your Own Self-Driving Car, For Free},
  journal = {IEEE Spectrum},
  year = {2012},
  url = {https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/sebastian-thrun-will-teach-you-how-to-build-your-own-selfdriving-car-for-free},
  note = {Accessed: 2024-05-25}
}

@article{gibson,
  title = {Broadband blues},
  author = {{The Economist}},
  journal = {The Economist},
  year = {2001},
  url = {https://www.economist.com/business/2001/06/21/broadband-blues},
  note = {Accessed: 2024-05-25}
}

@article{agarwal2016moocs,
  title={MOOCs and the Global Democratization of Higher Education},
  author={Agarwal, Anant},
  journal={The EvoLLLution},
  year={2016},
  month={June 24},
  url={https://evolllution.com/attracting-students/accessibility/moocs-and-the-global-democratization-of-higher-education}
}


@article{kim2014moocs,
  title={8 Myths About MOOCs},
  author={Kim, Joshua},
  journal={Inside Higher Ed},
  year={2014},
  month={February 10},
  url={https://www.insidehighered.com/blogs/technology-and-learning/8-myths-about-moocs}
}


@article{adams2013moocs,
  title={Are MOOCs Really a Failure?},
  author={Adams, Susan},
  journal={Forbes},
  year={2013},
  month={December 11},
  url={https://www.forbes.com/sites/susanadams/2013/12/11/are-moocs-really-a-failure/?sh=6d7d505cdaa2}
}


@article{edsurge2014moocs,
  title={Insights and Trends That Make MOOCs Matter},
  author={EdSurge},
  journal={EdSurge},
  year={2014},
  month={August 4},
  url={https://www.edsurge.com/news/2014-08-04-insights-and-trends-that-make-moocs-matter}
}


@article{solomon2013moocs,
  title={An Early Report Card on MOOCs},
  author={Solomon, Erica},
  journal={The Wall Street Journal},
  year={2013},
  month={October 8},
  url={https://www.wsj.com/articles/SB10001424052702303759604579093400834738972}
}

@article{ho2014harvardx,
  title={HarvardX and MITx: The first year of open online courses, Fall 2012-Summer 2013},
  author={Ho, Andrew D and Reich, Justin and Nesterko, Sergiy and Seaton, Daniel T and Mullaney, Tommy and Waldo, Jim and Chuang, Isaac},
  journal={HarvardX and MITx Working Paper No. 1},
  year={2014},
  month={January 21},
  note={Available at SSRN: \url{https://ssrn.com/abstract=2381263} or \url{http://dx.doi.org/10.2139/ssrn.2381263}}
}


@article{bates2014moocs,
  title={The strengths and weaknesses of MOOCs (Part I)},
  author={Bates, Tony},
  journal={Tony Bates},
  year={2014},
  month={October},
  url={https://www.tonybates.ca/2014/10/19/the-strengths-and-weaknesses-of-moocs-part-i/}
}


@inproceedings{Bali2014MOOCPG,
  title={MOOC Pedagogy: Gleaning Good Practice from Existing MOOCs},
  author={Maha Bali},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:2590255}
}

@misc{merriamWebsterAPI,
  author = {Merriam-Webster},
  title = {Merriam-Webster Spanish-English Dictionary API Documentation},
  year = {2024},
  url = {https://dictionaryapi.com/products/api-spanish-dictionary},
  note = {Accessed: 2024-05-24}
}


@misc{mochiAPI,
  author = {Mochi},
  title = {Mochi API Documentation},
  year = {2024},
  url = {https://mochi.cards/docs/api/},
  note = {Accessed: 2024-05-24}
}

@article{Alahmadi2020ExploringTE,
  title={Exploring the effect of lexical inferencing and dictionary consultation on undergraduate EFL students’ vocabulary acquisition},
  author={Alaa Alahmadi and Anouschka Foltz},
  journal={PLoS ONE},
  year={2020},
  volume={15},
  url={https://api.semanticscholar.org/CorpusID:220889377}
}

@article{bilingual_dictionary,
author = {Laufer, Batia and Hadar, Linor},
year = {1997},
month = {06},
pages = {},
title = {Assessing the Effectiveness of Monolingual, Bilingual, and “Bilingualised” Dictionaries in the Comprehension and Production of New Words},
volume = {81},
journal = {The Modern Language Journal},
doi = {10.1111/j.1540-4781.1997.tb01174.x},
url= {https://www.researchgate.net/publication/262907003_Assessing_the_Effectiveness_of_Monolingual_Bilingual_and_Bilingualised_Dictionaries_in_the_Comprehension_and_Production_of_New_Words}
}

@article{dictionaryvalue,
  title={Dictionary Use While Reading: The Effects On Comprehension and Vocabulary Acquisition For Students Of Different Verbal Abilities},
  author={Susan Knight},
  journal={The Modern Language Journal},
  year={1994},
  volume={78},
  pages={285-299},
  url={https://www.jstor.org/stable/330108}
}

@book{dictionarypitfalls,
author = {Nesi, Hilary},
year = {2000},
month = {02},
pages = {},
title = {The Use and Abuse of EFL Dictionaries},
isbn = {3-484-30998-9}, 
url = {https://cronfa.swan.ac.uk/Record/cronfa42801/Download/0042801-02082018162522.pdf}
}

@article{vocabcorrelatesskill,
author = {Laufer, Batia and Nation, Paul},
year = {1995},
month = {09},
pages = {307-322},
title = {Vocabulary Size and Use: Lexical Richness in L2 Written Production},
volume = {16},
journal = {Applied Linguistics - APPL LINGUIST},
doi = {10.1093/applin/16.3.307},
url = {https://www.researchgate.net/publication/249237693_Vocabulary_Size_and_Use_Lexical_Richness_in_L2_Written_Production}
}

@article{imagesdictionary,
title = {Using the keyword method in the classroom: Is the interacting imagery necessary?},
journal = {System},
volume = {45},
pages = {17-26},
year = {2014},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2014.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X14000712},
author = {Dacian D. Dolean},
keywords = {Keyword method, Foreign language, Vocabulary learning, Imagery, Mnemonics},
abstract = {The Keyword Method (KWM) is one of the most extensively researched methods used in foreign language vocabulary learning. However, evidence for its successful classroom application is limited and research results are mixed. This paper reports two experiments using a simple teacher-friendly procedure in which assessment of the KWM on Romanian speaking subjects was used for the first time. Results indicate that showing simultaneously a picture for a new L2 vocabulary word and one for the keyword increases retention, even when pictures show no interaction of keyword and target word meanings when applied in elementary (studies 1 and 2) and middle school classrooms (study 2). The outcome of this research may benefit teachers and support immediate and long-term efficiency of the KWM.}
}

@article{clockworkorange,
title = {Vocabulary learning and reading},
journal = {System},
volume = {6},
number = {2},
pages = {72-78},
year = {1978},
issn = {0346-251X},
doi = {https://doi.org/10.1016/0346-251X(78)90027-1},
url = {https://www.wgtn.ac.nz/lals/resources/paul-nations-resources/paul-nations-publications/publications/documents/1978-Saragi-Clockwork-orange.pdf},
author = {T. Saragi and I.S.P. Nation and G.F. Meister}
}

@InProceedings{tooltipdictionary,
author    = {Guruprakash, J. and Krithika, L.B.},
title     = {TOOLTIP TRANSLATOR FOR ELEARNING COURSEWARE TO IMPROVE LEARNER’S VOCABULARY},
series    = {4th International Conference on Education and New Learning Technologies},
booktitle = {EDULEARN12 Proceedings},
isbn      = {978-84-695-3491-5},
issn      = {2340-1117},
publisher = {IATED},
location  = {Barcelona, Spain},
month     = {2-4 July, 2012},
year      = {2012},
pages     = {1406-1413}}

@article{dictionaryimportance,
author = {Luppescu, Stuart and Day, Richard R.},
title = {Reading, Dictionaries, and Vocabulary Learning},
journal = {Language Learning},
volume = {43},
number = {2},
pages = {263-279},
doi = {https://doi.org/10.1111/j.1467-1770.1992.tb00717.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-1770.1992.tb00717.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-1770.1992.tb00717.x},
abstract = {This article focuses on the contribution to vocabulary learning of the use of bilingual dictionaries during reading by 293 Japanese university students studying English as a foreign language. The results of the study show that students who used a dictionary scored significantly better on a vocabulary test than students who did not use a dictionary. However, evidence appeared for differential item functioning: Some items were harder for the group that used dictionaries. A possible explanation for this tendency is that students who were unable to locate the appropriate gloss in the dictionary were misled as to the meaning of the word. Moreover, students who used a dictionary read nearly half as quickly as the group that did not use dictionaries.},
year = {1993}
}

@article{ghimire2019comparative,
  title={Comparative Study on Python Web Frameworks: Flask and Django},
  author={Ghimire, Devndra},
  journal={International Journal of Scientific Research in Computer Science, Engineering and Information Technology},
  volume={5},
  number={2},
  pages={317-323},
  year={2019},
  publisher={IJSRCSEIT}
}


@article{chineseboundary,
author = {Gao, Jianfeng and Li, Mu and Huang, Changning and Wu, Andi},
year = {2005},
month = {12},
pages = {531-574},
title = {Chinese Word Segmentation and Named Entity Recognition: A Pragmatic Approach},
volume = {31},
journal = {Computational Linguistics},
doi = {10.1162/089120105775299177}
}

@article{cepedasrs,
author = {Nicholas J. Cepeda and Edward Vul and Doug Rohrer and John T. Wixted and Harold Pashler},
title ={Spacing Effects in Learning: A Temporal Ridgeline of Optimal Retention},

journal = {Psychological Science},
volume = {19},
number = {11},
pages = {1095-1102},
year = {2008},
doi = {10.1111/j.1467-9280.2008.02209.x},
    note ={PMID: 19076480},

URL = { 
    
        https://doi.org/10.1111/j.1467-9280.2008.02209.x
    
    

},
eprint = { 
    
        https://doi.org/10.1111/j.1467-9280.2008.02209.x
    
    

}
,
    abstract = { To achieve enduring retention, people must usually study information on multiple occasions. How does the timing of study events affect retention? Prior research has examined this issue only in a spotty fashion, usually with very short time intervals. In a study aimed at characterizing spacing effects over significant durations, more than 1,350 individuals were taught a set of facts and—after a gap of up to 3.5 months—given a review. A final test was administered at a further delay of up to 1 year. At any given test delay, an increase in the interstudy gap at first increased, and then gradually reduced, final test performance. The optimal gap increased as test delay increased. However, when measured as a proportion of test delay, the optimal gap declined from about 20 to 40\% of a 1-week test delay to about 5 to 10\% of a 1-year test delay. The interaction of gap and test delay implies that many educational practices are highly inefficient. }
}


@article{kangsrs,
author = {Kang, Sean},
year = {2016},
month = {01},
pages = {},
title = {Spaced Repetition Promotes Efficient and Effective Learning: Policy Implications for Instruction},
volume = {3},
journal = {Policy Insights from the Behavioral and Brain Sciences},
doi = {10.1177/2372732215624708}
}

@misc{mlq2023tokens,
  title={Tokens, Context Windows \& Large Language Models (LLMs)},
  author={{MLQ.ai}},
  year={2023},
  url={https://www.mlq.ai/tokens-context-window-llms/}
}

@misc{liu2023lost,
      title={Lost in the Middle: How Language Models Use Long Contexts}, 
      author={Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
      year={2023},
      eprint={2307.03172},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{pilán2016readable,
      title={A Readable Read: Automatic Assessment of Language Learning Materials based on Linguistic Complexity}, 
      author={Ildikó Pilán and Sowmya Vajjala and Elena Volodina},
      year={2016},
      eprint={1603.08868},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{caines2023application,
      title={On the application of Large Language Models for language teaching and assessment technology}, 
      author={Andrew Caines and Luca Benedetto and Shiva Taslimipoor and Christopher Davis and Yuan Gao and Oeistein Andersen and Zheng Yuan and Mark Elliott and Russell Moore and Christopher Bryant and Marek Rei and Helen Yannakoudakis and Andrew Mullooly and Diane Nicholls and Paula Buttery},
      year={2023},
      eprint={2307.08393},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@book{
    Lave_Wenger_1991,
    place={Cambridge},
    series={Learning in Doing: Social, Cognitive and Computational Perspectives}, 
    title={Situated Learning: Legitimate Peripheral Participation},
    publisher={Cambridge University Press},
    author={Lave, Jean and Wenger, Etienne},
    year={1991},
    collection={Learning in Doing: Social, Cognitive and Computational Perspectives}
}

@article{krashen_2004,
author = {Krashen, Stephen},
year = {2004},
month = {01},
title = {The Power of Reading: Insights from the Research}
}

@article{hu_2000,
author = {Hu, Marcella and Nation, Paul},
year = {2000},
month = {01},
title = {Unknown Vocabulary Density and Reading Comprehension},
volume = {13},
journal = {Reading in a Foreign Language}
}

@article{nation1992vocabulary,
  title={What Vocabulary Size Is Needed to Read Unsimplified Texts for Pleasure?},
  author={Nation, Paul and Hirsch, David},
  journal={Reading in a Foreign Language},
  volume={8},
  number={2},
  pages={689--696},
  year={1992}
}
@inproceedings{stajner-2021-automatic,
    title = "Automatic Text Simplification for Social Good: Progress and Challenges",
    author = "Stajner, Sanja",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.233",
    doi = "10.18653/v1/2021.findings-acl.233",
    pages = "2637--2652",
}

@article{xu-etal-2016-optimizing,
    title = "Optimizing Statistical Machine Translation for Text Simplification",
    author = "Xu, Wei  and
      Napoles, Courtney  and
      Pavlick, Ellie  and
      Chen, Quanze  and
      Callison-Burch, Chris",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q16-1029",
    doi = "10.1162/tacl_a_00107",
    pages = "401--415",
    abstract = "Most recent sentence simplification systems use basic machine translation models to learn lexical and syntactic paraphrases from a manually simplified parallel corpus. These methods are limited by the quality and quantity of manually simplified corpora, which are expensive to build. In this paper, we conduct an in-depth adaptation of statistical machine translation to perform text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task.",
}

@inproceedings{sulem-etal-2018-semantic,
    title = "Semantic Structural Evaluation for Text Simplification",
    author = "Sulem, Elior  and
      Abend, Omri  and
      Rappoport, Ari",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1063",
    doi = "10.18653/v1/N18-1063",
    pages = "685--696",
    abstract = "Current measures for evaluating text simplification systems focus on evaluating lexical text aspects, neglecting its structural aspects. In this paper we propose the first measure to address structural aspects of text simplification, called SAMSA. It leverages recent advances in semantic parsing to assess simplification quality by decomposing the input based on its semantic structure and comparing it to the output. SAMSA provides a reference-less automatic evaluation procedure, avoiding the problems that reference-based methods face due to the vast space of valid simplifications for a given sentence. Our human evaluation experiments show both SAMSA{'}s substantial correlation with human judgments, as well as the deficiency of existing reference-based measures in evaluating structural simplification.",
}

@misc{zhang2017sentence,
      title={Sentence Simplification with Deep Reinforcement Learning}, 
      author={Xingxing Zhang and Mirella Lapata},
      year={2017},
      eprint={1703.10931},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{feng2023sentence,
      title={Sentence Simplification via Large Language Models}, 
      author={Yutao Feng and Jipeng Qiang and Yun Li and Yunhao Yuan and Yi Zhu},
      year={2023},
      eprint={2302.11957},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
} 

@misc{kew2023bless,
      title={BLESS: Benchmarking Large Language Models on Sentence Simplification}, 
      author={Tannon Kew and Alison Chi and Laura Vásquez-Rodríguez and Sweta Agrawal and Dennis Aumiller and Fernando Alva-Manchego and Matthew Shardlow},
      year={2023},
      eprint={2310.15773},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wu2024indepth,
      title={An In-depth Evaluation of GPT-4 in Sentence Simplification with Error-based Human Assessment}, 
      author={Xuanxin Wu and Yuki Arase},
      year={2024},
      eprint={2403.04963},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Murre2015ReplicationAA,
  title={Replication and Analysis of Ebbinghaus’ Forgetting Curve},
  author={Jaap M. J. Murre and Joeri Dros},
  journal={PLoS ONE},
  year={2015},
  volume={10},
  url={https://api.semanticscholar.org/CorpusID:9151801}
}

@article{bullaughey2019reading,
  title={Reading is a lot like spaced repetition, only better},
  author={Bullaughey, K.},
  year={2019},
  url={https://www.hackingchinese.com/reading-is-a-lot-like-spaced-repetition-only-better/},
  journal={Hacking Chinese},
  note={Retrieved from \url{https://www.hackingchinese.com/reading-is-a-lot}}
}

@article{krashenreview,
author = {Luo, Zixu},
year = {2024},
month = {03},
pages = {130-135},
title = {A Review of Krashen’s Input Theory},
volume = {26},
journal = {Journal of Education, Humanities and Social Sciences},
doi = {10.54097/3fnf5786}
}

@article{karasimos2022battle,
  title={The battle of language learning apps: a cross-platform overview},
  author={Karasimos, Athanasios},
  journal={Research Papers in Language Teaching and Learning},
  volume={12},
  number={1},
  pages={150--166},
  year={2022},
  publisher={Hellenic Open University}
}

@article{Liu2015AnAO,
  title={An Analysis of Social Network Websites for Language Learning: Implications for Teaching and Learning English as a Second Language},
  author={Min Liu and Kana Abe and Mengwen Cao and Sa Liu and Duygu Uslu Ok and Jeong-bin Hannah Park and Claire Meadows Parrish and Veronica Gabriela Sardegna},
  journal={the CALICO Journal},
  year={2015},
  volume={32},
  pages={114-152},
  url={https://api.semanticscholar.org/CorpusID:54206295}
}

@article{Chang2015ImprovingRR,
  title={Improving reading rates and comprehension through audio-assisted extensive reading for beginner learners},
  author={Anna C.-S. Chang and Sonia Millett},
  journal={System},
  year={2015},
  volume={52},
  pages={91-102},
  url={https://api.semanticscholar.org/CorpusID:60160153}
}

@article{Nation2020GradedRA,
  title={Graded Readers and Vocabulary},
  author={Paul Nation and Karen Wang Ming-Tzu},
  journal={Reading in a foreign language},
  year={2020},
  volume={12},
  url={https://api.semanticscholar.org/CorpusID:60539916}
}

@misc{TheChairmansBao_AI_2023,
    author = {{The Chairman's Bao}},
    title = {AI Apps to Help You Learn Chinese},
    year = {2023},
    url = {https://www.thechairmansbao.com/blog/ai-apps-learn-chinese/},
    note = {Accessed: 2024-05-19}
}

@inproceedings{mcdonald2016,
author = {Macdonald, Kyle and Frank, Michael},
year = {2016},
month = {08},
pages = {},
title = {When does passive learning improve the effectiveness of active learning?}
}

@misc{SuperMemo_Method_2023,
    author = {{SuperMemo World}},
    title = {The SuperMemo Method},
    year = {2023},
    url = {https://www.supermemo.com/en/supermemo-method},
    note = {Accessed: 2024-05-19}
}

@misc{essay89410,
            year = {2022},
           month = {February},
          author = {A.E. {Beursgens}},
           title = {A Markov process analysis of and a proposal for adjustments to the Leitner system},
             url = {http://essay.utwente.nl/89410/},
        abstract = {The Leitner system is a review scheme for flashcards that uses spaced repetition. Although there has been quite some research performed in ways to implement spaced repetition on computers, no existing simple mathematical model for the physical use of the Leitner system could be found. The Leitner system is modelled as a discrete-time inhomogeneous Markov process. The system is analysed using three performance measures, taking into account the efficiency of the system and the distribution of the workload. The relation between the number of words in the system and both the number of words mastered after as well as reviewed on day $t$ is reasoned to be linear. Moreover, the ratio between the number of words reviewed on two days of a cycle is shown to be fixed. Furthermore, the influence of the global difficulty on the results is examined. Several adjustments to the Leitner system are proposed to make the system more user friendly, both in equalizing the distribution of workload over days and in adjusting it to weekly cycles. Finally, the lessons learned from those adjustments are used to propose an alternative to the Leitner system. However, the influence of the adjustments to the system on the long-term retention of the studied words could not be examined and needs further research.}
}

@inproceedings{shortestpathrepetitionscheduling,
author = {Ye, Junyao and Su, Jingyong and Cao, Yilong},
title = {A Stochastic Shortest Path Algorithm for Optimizing Spaced Repetition Scheduling},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539081},
doi = {10.1145/3534678.3539081},
abstract = {Spaced repetition is a mnemonic technique where long-term memory can be efficiently formed by following review schedules. For greater memorization efficiency, spaced repetition schedulers need to model students' long-term memory and optimize the review cost. We have collected 220 million students' memory behavior logs with time-series features and built a memory model with Markov property. Based on the model, we design a spaced repetition scheduler guaranteed to minimize the review cost by a stochastic shortest path algorithm. Experimental results have shown a 12.6\% performance improvement over the state-of-the-art methods. The scheduler has been successfully deployed in the online language-learning app MaiMemo to help millions of students.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4381–4390},
numpages = {10},
keywords = {spaced repetition, optimal control, language learning},
location = {Washington DC, USA},
series = {KDD '22}
}
